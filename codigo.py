# -*- coding: utf-8 -*-
"""codigo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QzBo9KYB5Rw3HXHJzWQLOtKuTWXd8xOM

# Código da rede neural:
"""

import random
import numpy as np

class Network (object):
  def __init__(self, sizes):
    """
    sizes é uma lista cujos elementos representam o número de neurônios em cada camada da rede. Se a lista é [3,5,1],
    então a primeira camada tem 3 neurônios, a segunda tem 5 e a terceira tem 1. Os pesos e biases são inicializados
    aleatoriamente com uma distribuição gaussiana de média 0 e variância 1.
    """
    self.num_camadas = len(sizes)
    self.sizes = sizes
    self.biases = [np.random.randn(y,1) for y in sizes[1:]]
    self.pesos = [np.random.randn(y,x) for x,y in zip(sizes[:-1], sizes[1:])]

  def feedforward(self, a):
    for b,w in zip(self.biases, self.pesos):
      a = sigmoid(np.dot(w,a)+b)
    return a

  def SGD(self, dados_treinamento, epochs, mini_batch_size, eta, dados_teste=None):
    """
    Treina a rede neural utilizando o gradiente descendente estocástico com mini-batches. Os dados de treinamento é
    uma lista de tuplas (x,y) que representam as entradas de treinamento e as saídas desejadas. A variável epochs é
    a quantidade de epochs de treinamento. A variável mini_batch_size é o tamanho dos mini-batches que serão usados
    na amostragem. A variável eta é a taxa de aprendizado. Se dados_teste forem fornecidos, a rede será avaliada
    com os dados de teste após cada epoch e mostrará o progresso parcial - o que pode ser útil, mas torna o
    processo consideravelmente mais lento.
    """

    if dados_teste:
      n_test = len(dados_teste)

    n = len(dados_treinamento) #(entradas,resultados)

    for j in range(epochs):
      random.shuffle(dados_treinamento)
      mini_batches = [dados_treinamento[k:k+mini_batch_size] for k in range(0,n,mini_batch_size)]

      for mini_batch in mini_batches:
        self.atualiza_mini_batch(mini_batch, eta)
      if dados_teste:
        print("Epoch {0}: {1} / {2}".format(j,self.evaluate(dados_teste), n_test))
      else:
        print("Epoch {0} completa".format(j))

  def evaluate(self, dados_teste):
    """
    Retorna o número de entradas de teste para os quais a rede neural dá a resposta correta.
    """
    resultados_teste = [(np.argmax(self.feedforward(x)), y) for (x, y) in dados_teste]
    return sum(int(x == y) for (x, y) in resultados_teste)

  def atualiza_mini_batch(self, mini_batch, eta):
    """
    Atualiza os pesos e biases da rede: aplica o gradiente descendente com backpropagation em um mini-batch.
    A variável mini_batch é uma lista de tuplas (x,y). A variável eta é a taxa de aprendizado.
    """

    # Cria lista de zeros para biases e pesos de todos os neurônios
    nabla_b = [np.zeros(b.shape) for b in self.biases]
    nabla_w = [np.zeros(w.shape) for w in self.pesos]

    for x,y in mini_batch:
      delta_nabla_b, delta_nabla_w = self.backprop(x, y)
      nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
    nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]

    self.pesos = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.pesos, nabla_w)]
    self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]
  
  def backprop(self, x, y):
    """
    Retorna uma tupla (nabla_b, nabla_w) que representa o gradiente da função de custo.
    """
    nabla_b = [np.zeros(b.shape) for b in self.biases]
    nabla_w = [np.zeros(w.shape) for w in self.pesos]

    #feedforward
    ativacao = x #entradas de treinamento que passam pela função sigmoid
    ativacoes = [x] #armazena cada ativacao (todas são sigmoid)
    zs = [] #armazena cada ativacao

    for b, w in zip(self.biases, self.pesos):
      z = np.dot(w, ativacao)+b 
      zs.append(z)
      ativacao = sigmoid(z)
      ativacoes.append(ativacao)

    #backward
    delta = self.custo_derivada(ativacoes[-1], y) * sigmoid_derivada(zs[-1])
    nabla_b[-1] = delta
    nabla_w[-1] = np.dot(delta, ativacoes[-2].transpose())

    for l in range(2, self.num_camadas):
      z = zs[-l] #pega o último valor armazenado
      ds = sigmoid_derivada(z)
      delta = np.dot(self.pesos[-l+1].transpose(), delta) * ds
      nabla_b[-l] = delta
      nabla_w[-l] = np.dot(delta, ativacoes[-l-1].transpose())

    return (nabla_b, nabla_w)

  def custo_derivada(self, saida_ativacoes, y):
    """
    Retorna o vetor de derivadas parciais d(custo)/d(ativacao).
    """
    return (saida_ativacoes - y) #é mais um erro entre saída e valor esperado

def sigmoid(z):
  return 1.0/(1.0+np.exp(-z))

def sigmoid_derivada(z):
  return sigmoid(z) * (1 - sigmoid(z))

"""# Código para carregamento de dados:

Para que o código funcione adequadamente:
*  Crie uma pasta chamada ea006 em seu próprio Google Drive;
*  Faça uma cópia deste Google Colab dentro da pasta;
*  Baixe o arquivo "mnist.pkl.gz" em http://www.iro.umontreal.ca/~lisa/deep/data/mnist/
*  Suba o arquivo para a pasta "ea006" criada.

Isso é necessário porque o código buscará os dados em /content/drive/MyDrive/ea006.
"""

#mnist_loader.py
import _pickle as cPickle
import gzip
from google.colab import drive

import numpy as np
import matplotlib.cm as cm
import matplotlib.pyplot as plt

def load_data():
    """
    Retorna os dados do conjunto MNIST como uma tupla que contém
    dados de treinamento, dados de validação e dados de teste.

    Temos as variáveis: training_data, validation_data e test_data.

    -> training_data:
    A função retorna a variável training_data como uma tupla de duas entradas.

    A primeira entrada contém as imagens de treinamento.
    É um numpy ndarray com 50 mil entradas, cada uma contendo 784 valores,
    já que cada imagem é de 28x28 pixels.

    A segunda entrada também é um numpy ndarray com 50 mil entradas,
    porém seu conteúdo são apenas dígitos de 0 a 9 correspondentes
    a cada imagem. São os rótulos de cada imagem, o "gabarito".

    -> validation_data e test_data:
    É a mesma coisa de training_data, porém cada conjunto possui 10 mil imagens.
    """

    #subir o google drive para o colab
    drive.mount('/content/drive')

    #caminho do diretório para o conjunto de dados
    DATA_PATH = ("/content/drive/MyDrive/ea006/")

    #descompactar e descarregar o arquivo mnist.pkl.gz nas variáveis descritas acima
    f = gzip.open(DATA_PATH+'mnist.pkl.gz')
    training_data, validation_data, test_data = cPickle.load(f, encoding='latin1')
    f.close()

    return (training_data, validation_data, test_data)

def load_data_wrapper():
    """
    Retorna as tuplas training_data e test_data.
    Baseia-se na função load_data(), porém modifica o formato
    convenientemente para a implementação da rede neural.

    -> training_data:
    É uma lista com 60 mil tuplas (x,y), onde:
      -> x é um numpy ndarray de tamanho 784, que contém a imagem de entrada;
      -> y é um numpy ndarray que representa o vetor que corresponde ao dígito
        correto para x.
      Por exemplo, se a imagem é um 9, y será (0,0,0,0,0,0,0,0,0,1).

    -> test_data:
    São listas com 10 mil tuplas (x,y), onde:
      -> x é um numpy ndarray de tamanho 784, que contém a imagem de entrada;
      -> y é um inteiro com o valor que corresponde ao dígito correto para x.
      Por exemplo, se a imagém é um 9, y será um 9.

    Esses formatos são mais convenientes para a rede neural desenvolvida.
    """
    #carrega os dados de mnist.pkl
    tr_d, va_d, te_d = load_data()

    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]] # 784 entradas de profundidade 1 (escala de cinza)
    training_results = [vectorized_result(y) for y in tr_d[1]] #transforma em vetor de tamanho 10
    training_data = list(zip(training_inputs, training_results)) #lista de tuplas com inputs e respectivos rótulos

    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]
    validation_results = [vectorized_result(y) for y in va_d[1]]
    validation_data = list(zip(validation_inputs, validation_results))

    # não consegui usar cPickle.load de maneira a separar os dados em apenas training_data e test_data.
    # então juntei o conjunto validation_data ao training_data, obtendo 60k imagens.
    training_data = training_data + validation_data

    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]
    test_data = list(zip(test_inputs, te_d[1]))

    print('quantidade de imagens de treinamento: '+str(len(training_data)))
    print('quantidade de imagens de teste: '+str(len(test_data)))

    rotulos = []
    plt.figure(figsize=(10, 10))
    print("\n- - - - - 10 amostras de rótulos e suas respectivas imagens - - - - -")
    for i in range(0,10):
      plt.subplot(1, 10, i+1)
      rand_i = np.random.randint(0,len(training_inputs))
      plt.imshow(training_inputs[rand_i].reshape((28,28)), cmap=cm.Greys_r)
      rotulo = tr_d[1][rand_i]
      rotulos.append(rotulo)
    print("\nRÓTULOS:")
    print(rotulos)
    print("\nIMAGENS:")
    
    return (training_data, test_data)

def vectorized_result(j):
    """
    Converte dígitos de 0 a 9 em um vetor de tamanho 10
    com o valor 1.0 na j-ésima posição e zeros nas demais posições.
    Retorna o vetor.
    """
    e = np.zeros((10, 1))
    e[j] = 1.0
    return e

"""# Carregar os dados:"""

dados_treinamento, dados_teste = load_data_wrapper()

"""# Treinar e testar a rede:"""

net = Network([784, 30, 10])
net.SGD(dados_treinamento, 40, 10, 3, dados_teste=dados_teste) #dados_treinamento, epochs, mini_batch_size, eta, dados_teste)

"""# Reconstruir o que foi feito até agora, mas com a biblioteca:

**Importar a biblioteca:**
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.utils import to_categorical

"""**Carregar dados:**"""

# carregar os dados

(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

x_train = x_train.reshape(60000, 784).astype("float32") / 255
x_test = x_test.reshape(10000, 784).astype("float32") / 255

rotulos = []
plt.figure(figsize=(10, 10))
print("\n- - - - - 10 amostras de rótulos e suas respectivas imagens - - - - -")
for i in range(0,10):
  plt.subplot(1, 10, i+1)
  rand_i = np.random.randint(0,len(x_train))
  plt.imshow(x_train[rand_i].reshape((28,28)), cmap=cm.Greys_r)
  rotulo = int(y_train[rand_i])
  rotulos.append(rotulo)
print("\nRÓTULOS:")
print(rotulos)
print("\nIMAGENS:")


# converter os valores do "gabarito" para vetor correspondente
# por exemplo, se o valor é um 9, o vetor será [0,0,0,0,0,0,0,0,0,1]

num_classes = 10 

y_train = to_categorical(y_train, num_classes) 
y_test = to_categorical(y_test, num_classes)

"""**Criar o modelo de rede neural:**

As funções utilizadas estão documentadas em:
*  https://keras.io/api/layers/core_layers/input/
*  https://keras.io/api/layers/core_layers/dense/
*  https://keras.io/api/models/model/#model-class
*  https://keras.io/api/models/model_training_apis/
*  https://keras.io/api/losses/probabilistic_losses/#categoricalcrossentropy-class
*  https://keras.io/api/optimizers/sgd/
*  https://keras.io/api/metrics/accuracy_metrics/#categoricalaccuracy-class
"""

keras.backend.clear_session()

# fazer o design da rede neural

# camada de entrada
inputs = keras.Input(shape=(784,)) # 784 inputs

initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)

# Dense é um tipo de camada onde todos os neurônios são conectados. aqui temos 30 neurônios.
dense1 = layers.Dense(units=30, activation="sigmoid", kernel_initializer=initializer, bias_initializer=initializer)

#camada oculta
camada_oculta = dense1(inputs) # contém as saídas da camada criada acima.

# outra camada do tipo Dense, agora com 10 neurônios.
dense2 = layers.Dense(units=10, activation="sigmoid", kernel_initializer=initializer, bias_initializer=initializer) 

# camada de saída
outputs = dense2(camada_oculta) # outputs contém as saídas da camada criada acima.

# criar o modelo a partir do design descrito
model = keras.Model(inputs=inputs, outputs=outputs, name="modelo") 
model.summary() # imprime o sumário do modelo

# configura alguns hiperparâmetros do modelo

model.compile(
    # calcula a quantidade que um modelo deve procurar minimizar durante o treinamento
    loss=keras.losses.CategoricalCrossentropy(from_logits=True),
    # otimizador de gradiente descendente
    optimizer=keras.optimizers.SGD(learning_rate=3),
    # avalia o desempenho do modelo ao verificar a taxa de acerto das predições (comparando com os rótulos)
    metrics=["categorical_accuracy"],
)

"""**Treinar e testar o modelo:**"""

# treina o modelo

model.fit(x_train, y_train, batch_size=10, epochs=40, validation_split=0.0)

# retorna valores de loss e metrics (definidos em model.compile)

test_scores = model.evaluate(x_test, y_test, verbose=1)
print("Test loss:", test_scores[0])
print("Test accuracy:", test_scores[1])

"""# Rede neural convolucional

**Carregar dados:**
"""

# carregar os dados

(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# perceba como modificamos as entradas: não é mais um array de 784 neurônios.
# agora temos uma matriz 28x28 de profundidade 1

x_train = x_train.reshape(60000, 28, 28, 1).astype("float32") / 255
x_test = x_test.reshape(10000, 28, 28, 1).astype("float32") / 255

rotulos = []
plt.figure(figsize=(10, 10))
print("\n- - - - - 10 amostras de rótulos e suas respectivas imagens - - - - -")
for i in range(0,10):
  plt.subplot(1, 10, i+1)
  rand_i = np.random.randint(0,len(x_train))
  plt.imshow(x_train[rand_i].reshape((28,28)), cmap=cm.Greys_r)
  rotulo = int(y_train[rand_i])
  rotulos.append(rotulo)
print("\nRÓTULOS:")
print(rotulos)
print("\nIMAGENS:")

# converter os valores do "gabarito" para vetor correspondente
# por exemplo, se o valor é um 9, o vetor será [0,0,0,0,0,0,0,0,0,1]

num_classes = 10 

y_train = to_categorical(y_train, num_classes) 
y_test = to_categorical(y_test, num_classes)

"""**Criar o modelo de rede neural convolucional:**

As funções utilizadas estão documentadas em:
*  https://keras.io/guides/sequential_model/
*  https://keras.io/api/layers/core_layers/input/
*  https://keras.io/api/layers/convolution_layers/convolution2d/
*  https://keras.io/api/layers/pooling_layers/max_pooling2d/
*  https://keras.io/api/layers/reshaping_layers/flatten/
*  https://keras.io/api/layers/regularization_layers/dropout/
*  https://keras.io/api/layers/core_layers/dense/

Encontramos diversas ferramentas e métodos descritos no relatório, como funções de ativação, inicialização de parâmetros, regularização etc no link: https://keras.io/api/layers/
"""

keras.backend.clear_session()

# fazer o design da rede neural
# vamos utilizar o modelo "sequential"


model = keras.Sequential(
    [
        keras.Input(shape=(28, 28, 1)),
        layers.Conv2D(filters=32, kernel_size=(3, 3), activation="relu", padding='same'),
        layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)),
        layers.Conv2D(filters=32, kernel_size=(3, 3), activation="relu", padding='same'),
        layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)),
        layers.Flatten(),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation="softmax"),
    ]
)

model.summary() # imprime o sumário do modelo

# configura alguns hiperparâmetros do modelo

model.compile(
    loss=keras.losses.CategoricalCrossentropy(), #  computa a quantidade que um modelo deve procurar minimizar durante o treinamento
    optimizer=keras.optimizers.Adam(learning_rate=0.01), # otimizador de gradiente descendente
    metrics=["accuracy"], # avalia o desempenho do modelo ao verificar a taxa de acerto das predições (comparando com os rótulos)
)

"""**Treinar e testar o modelo:**"""

# treina o modelo

model.fit(x_train, y_train, batch_size=128, epochs=10, validation_split=0.0)

# retorna valores de loss e metrics (definidos em model.compile)

test_scores = model.evaluate(x_test, y_test, verbose=1)
print("Test loss:", test_scores[0])
print("Test accuracy:", test_scores[1])

!python --version